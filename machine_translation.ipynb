{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "In this notebook, sections that end with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully!\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text.\n",
    "\n",
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  We'll be using a dataset we created for this project that contains a small vocabulary.  You'll be able to train your model in a reasonable time with this dataset.\n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "\n",
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 4, 'jumps': 6, 'jove': 11, 'quick': 2, 'sentence': 21, 'lexicography': 15, 'over': 7, 'a': 3, 'prize': 17, 'the': 1, 'lazy': 8, 'short': 20, 'by': 10, 'is': 19, 'this': 18, 'study': 13, 'fox': 5, 'of': 14, 'won': 16, 'dog': 9, 'my': 12}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "import project_tests as tests\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    return sequences, tokenizer\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x, maxlen=length, padding='post', truncating='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "print('Data Preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple function to print and compare translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_predictions(model, X, y, tokenizer, number_of_predictions=10):\n",
    "    '''\n",
    "    This function is used to print predictions made by the model followed\n",
    "    by the correct translation\n",
    "    \n",
    "    :param model: is the model that will make the predictions\n",
    "    :param X: preprocessed and padded inputs\n",
    "    :param y: correct sentences\n",
    "    :param number_of_predictions: number of predictions to be printed\n",
    "    '''\n",
    "    prediction_separator = '-------------------------------------------------------'\n",
    "    \n",
    "    for idx in range(number_of_predictions):\n",
    "        print(prediction_separator)\n",
    "        print()\n",
    "        print('Predicted sentence:\\n{0}'.format(logits_to_text(model.predict(X[idx:idx+1])[0], tokenizer)))\n",
    "        print()\n",
    "        print('Correct translation:\\n{0}'.format(y[idx:idx+1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difining files weights and models files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple RNN files\n",
    "simple_rnn_best_weights = 'simple_rnn_best_weights.hdf5'\n",
    "simple_rnn_file = 'simple_rnn.hdf5'\n",
    "\n",
    "# Embed RNN Files\n",
    "embed_rnn_best_weights = 'embed_rnn_best_weights.hdf5'\n",
    "embed_rnn_file = 'embed_rnn.hdf5'\n",
    "\n",
    "# Bidirectional RNN Files\n",
    "bd_rnn_best_weights = 'db_rnn_best_weights.hdf5'\n",
    "bd_rnn_file = 'bd_rnn.hdf5'\n",
    "\n",
    "# Autoencoder RNN Files\n",
    "encdec_rnn_best_weights = 'autoenc_rnn_best_weights.hdf5'\n",
    "encdec_rnn_file = 'autoenc_rnn.hdf5'\n",
    "\n",
    "# Final RNN Model Files\n",
    "final_rnn_best_weights = 'final_rnn_best_weights.hdf5'\n",
    "final_rnn_file = 'final_rnn.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 21, 14)            672       \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 21, 28)            3612      \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 21, 56)            14280     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 545)           31065     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 345)           188370    \n",
      "=================================================================\n",
      "Total params: 237,999\n",
      "Trainable params: 237,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 2.3584 - acc: 0.4909Epoch 00000: val_loss improved from inf to 1.58060, saving model to simple_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 19s - loss: 2.3541 - acc: 0.4914 - val_loss: 1.5806 - val_acc: 0.5925\n",
      "Epoch 2/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.5351 - acc: 0.5937Epoch 00001: val_loss improved from 1.58060 to 1.42322, saving model to simple_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 14s - loss: 1.5344 - acc: 0.5938 - val_loss: 1.4232 - val_acc: 0.6062\n",
      "Epoch 3/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.3823 - acc: 0.6155Epoch 00002: val_loss improved from 1.42322 to 1.32119, saving model to simple_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 14s - loss: 1.3820 - acc: 0.6155 - val_loss: 1.3212 - val_acc: 0.6145\n",
      "Epoch 4/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.3057 - acc: 0.6263Epoch 00003: val_loss improved from 1.32119 to 1.24672, saving model to simple_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 14s - loss: 1.3051 - acc: 0.6264 - val_loss: 1.2467 - val_acc: 0.6200\n",
      "Epoch 5/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.2493 - acc: 0.6349Epoch 00004: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.2491 - acc: 0.6349 - val_loss: 1.3491 - val_acc: 0.6089\n",
      "Epoch 6/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.2044 - acc: 0.6417Epoch 00005: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.2045 - acc: 0.6416 - val_loss: 1.3821 - val_acc: 0.6010\n",
      "Epoch 7/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.1715 - acc: 0.6461Epoch 00006: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.1713 - acc: 0.6461 - val_loss: 1.3355 - val_acc: 0.6139\n",
      "Epoch 8/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.1402 - acc: 0.6502Epoch 00007: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.1401 - acc: 0.6502 - val_loss: 1.4239 - val_acc: 0.6059\n",
      "Epoch 9/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.1194 - acc: 0.6525Epoch 00008: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.1192 - acc: 0.6526 - val_loss: 1.4407 - val_acc: 0.6029\n",
      "Epoch 10/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.1036 - acc: 0.6542Epoch 00009: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.1036 - acc: 0.6542 - val_loss: 1.3482 - val_acc: 0.6126\n",
      "Epoch 11/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0833 - acc: 0.6572Epoch 00010: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0833 - acc: 0.6572 - val_loss: 1.6083 - val_acc: 0.5927\n",
      "Epoch 12/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.6586Epoch 00011: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0706 - acc: 0.6587 - val_loss: 1.6392 - val_acc: 0.5962\n",
      "Epoch 13/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0593 - acc: 0.6605Epoch 00012: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0592 - acc: 0.6606 - val_loss: 1.6192 - val_acc: 0.6076\n",
      "Epoch 14/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0496 - acc: 0.6615Epoch 00013: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0499 - acc: 0.6614 - val_loss: 1.6552 - val_acc: 0.5929\n",
      "Epoch 15/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0326 - acc: 0.6644Epoch 00014: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0327 - acc: 0.6644 - val_loss: 1.6277 - val_acc: 0.5896\n",
      "Epoch 16/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6645Epoch 00015: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0297 - acc: 0.6646 - val_loss: 1.5279 - val_acc: 0.6038\n",
      "Epoch 17/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0217 - acc: 0.6660Epoch 00016: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0216 - acc: 0.6660 - val_loss: 1.6875 - val_acc: 0.5962\n",
      "Epoch 18/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0044 - acc: 0.6695Epoch 00017: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0043 - acc: 0.6695 - val_loss: 1.8293 - val_acc: 0.5915\n",
      "Epoch 19/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0035 - acc: 0.6700Epoch 00018: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 1.0034 - acc: 0.6701 - val_loss: 1.6381 - val_acc: 0.5957\n",
      "Epoch 20/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9956 - acc: 0.6718Epoch 00019: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9953 - acc: 0.6719 - val_loss: 1.6691 - val_acc: 0.5935\n",
      "Epoch 21/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9933 - acc: 0.6725Epoch 00020: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9933 - acc: 0.6724 - val_loss: 1.9044 - val_acc: 0.5797\n",
      "Epoch 22/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9862 - acc: 0.6748Epoch 00021: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9861 - acc: 0.6748 - val_loss: 1.5803 - val_acc: 0.5993\n",
      "Epoch 23/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.6769Epoch 00022: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9772 - acc: 0.6769 - val_loss: 1.7648 - val_acc: 0.5870\n",
      "Epoch 24/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9652 - acc: 0.6800Epoch 00023: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9651 - acc: 0.6801 - val_loss: 1.8375 - val_acc: 0.5852\n",
      "Epoch 25/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9683 - acc: 0.6800Epoch 00024: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9681 - acc: 0.6801 - val_loss: 1.8623 - val_acc: 0.5887\n",
      "Epoch 26/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9554 - acc: 0.6839Epoch 00025: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9553 - acc: 0.6839 - val_loss: 1.8342 - val_acc: 0.5813\n",
      "Epoch 27/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9508 - acc: 0.6856Epoch 00026: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9509 - acc: 0.6856 - val_loss: 1.8381 - val_acc: 0.5837\n",
      "Epoch 28/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9395 - acc: 0.6904Epoch 00027: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9395 - acc: 0.6904 - val_loss: 2.0637 - val_acc: 0.5720\n",
      "Epoch 29/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9394 - acc: 0.6914Epoch 00028: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9392 - acc: 0.6914 - val_loss: 1.9153 - val_acc: 0.5778\n",
      "Epoch 30/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9259 - acc: 0.6955Epoch 00029: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9261 - acc: 0.6955 - val_loss: 2.0155 - val_acc: 0.5792\n",
      "Epoch 31/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9188 - acc: 0.6976Epoch 00030: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9187 - acc: 0.6977 - val_loss: 1.9838 - val_acc: 0.5820\n",
      "Epoch 32/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9090 - acc: 0.7004Epoch 00031: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9092 - acc: 0.7004 - val_loss: 2.1507 - val_acc: 0.5655\n",
      "Epoch 33/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9074 - acc: 0.7005Epoch 00032: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9074 - acc: 0.7005 - val_loss: 2.1360 - val_acc: 0.5727\n",
      "Epoch 34/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7020Epoch 00033: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9045 - acc: 0.7020 - val_loss: 2.1253 - val_acc: 0.5759\n",
      "Epoch 35/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9048 - acc: 0.7006Epoch 00034: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.9049 - acc: 0.7005 - val_loss: 2.1823 - val_acc: 0.5659\n",
      "Epoch 36/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.7063Epoch 00035: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8864 - acc: 0.7063 - val_loss: 2.1799 - val_acc: 0.5684\n",
      "Epoch 37/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8897 - acc: 0.7051Epoch 00036: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8894 - acc: 0.7052 - val_loss: 2.2377 - val_acc: 0.5713\n",
      "Epoch 38/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8817 - acc: 0.7074Epoch 00037: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8818 - acc: 0.7073 - val_loss: 2.1908 - val_acc: 0.5661\n",
      "Epoch 39/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8793 - acc: 0.7079Epoch 00038: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8794 - acc: 0.7078 - val_loss: 2.2505 - val_acc: 0.5647\n",
      "Epoch 40/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8755 - acc: 0.7092Epoch 00039: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8755 - acc: 0.7093 - val_loss: 2.1893 - val_acc: 0.5642\n",
      "Epoch 41/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8776 - acc: 0.7085Epoch 00040: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8782 - acc: 0.7083 - val_loss: 2.1951 - val_acc: 0.5694\n",
      "Epoch 42/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8706 - acc: 0.7104Epoch 00041: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8703 - acc: 0.7104 - val_loss: 2.2309 - val_acc: 0.5730\n",
      "Epoch 43/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8681 - acc: 0.7109Epoch 00042: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8682 - acc: 0.7109 - val_loss: 2.2499 - val_acc: 0.5695\n",
      "Epoch 44/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8681 - acc: 0.7112Epoch 00043: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8679 - acc: 0.7113 - val_loss: 2.2576 - val_acc: 0.5678\n",
      "Epoch 45/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8585 - acc: 0.7133Epoch 00044: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8584 - acc: 0.7133 - val_loss: 2.2320 - val_acc: 0.5780\n",
      "Epoch 46/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8862 - acc: 0.7028Epoch 00045: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8861 - acc: 0.7027 - val_loss: 2.0526 - val_acc: 0.5806\n",
      "Epoch 47/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8785 - acc: 0.7051Epoch 00046: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8785 - acc: 0.7051 - val_loss: 2.2737 - val_acc: 0.5635\n",
      "Epoch 48/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8514 - acc: 0.7152Epoch 00047: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8515 - acc: 0.7151 - val_loss: 2.3225 - val_acc: 0.5665\n",
      "Epoch 49/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8737 - acc: 0.7080Epoch 00048: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8735 - acc: 0.7080 - val_loss: 2.1496 - val_acc: 0.5783\n",
      "Epoch 50/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8667 - acc: 0.7105Epoch 00049: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8666 - acc: 0.7105 - val_loss: 2.1978 - val_acc: 0.5801\n",
      "Epoch 51/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8453 - acc: 0.7173Epoch 00050: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8453 - acc: 0.7173 - val_loss: 2.3122 - val_acc: 0.5639\n",
      "Epoch 52/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8722 - acc: 0.7064Epoch 00051: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8720 - acc: 0.7065 - val_loss: 2.1013 - val_acc: 0.5725\n",
      "Epoch 53/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8597 - acc: 0.7116Epoch 00052: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8604 - acc: 0.7115 - val_loss: 2.1067 - val_acc: 0.5766\n",
      "Epoch 54/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.7170Epoch 00053: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8454 - acc: 0.7170 - val_loss: 2.2403 - val_acc: 0.5698\n",
      "Epoch 55/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8380 - acc: 0.7183Epoch 00054: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8378 - acc: 0.7184 - val_loss: 2.2752 - val_acc: 0.5722\n",
      "Epoch 56/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8568 - acc: 0.7138Epoch 00055: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8571 - acc: 0.7137 - val_loss: 2.1911 - val_acc: 0.5687\n",
      "Epoch 57/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8703 - acc: 0.7065Epoch 00056: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8703 - acc: 0.7066 - val_loss: 2.1991 - val_acc: 0.5641\n",
      "Epoch 58/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8357 - acc: 0.7193Epoch 00057: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8360 - acc: 0.7192 - val_loss: 2.3119 - val_acc: 0.5582\n",
      "Epoch 59/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8331 - acc: 0.7193Epoch 00058: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 14s - loss: 0.8328 - acc: 0.7194 - val_loss: 2.3551 - val_acc: 0.5578\n",
      "Epoch 60/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8317 - acc: 0.7193Epoch 00059: val_loss did not improve\n",
      "110288/110288 [==============================] - 14s - loss: 0.8315 - acc: 0.7193 - val_loss: 2.3189 - val_acc: 0.5646\n",
      "\n",
      "Loading best weights...\n",
      "Saving model for later use...\n",
      "\n",
      "Predicted sentence: new jersey est parfois chaud en l' mais il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation: [\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Build the layers\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    gru0 = GRU(14, return_sequences=True, dropout=0.1)(inputs)\n",
    "    gru1 = GRU(28, return_sequences=True, dropout=0.2)(gru0)\n",
    "    gru2 = GRU(56, return_sequences=True, dropout=0.4)(gru1)\n",
    "    dense0 = TimeDistributed(Dense(english_vocab_size + french_vocab_size, activation='tanh'))(gru2)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(dense0)\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1, # Adding 1 to avoid tokenizer error\n",
    "    len(french_tokenizer.word_index) + 1)  # Adding 1 to the other tokenized vector\n",
    "\n",
    "simple_rnn_model.summary()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=simple_rnn_best_weights, save_best_only=True, verbose=1)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences,\n",
    "                     batch_size=1024, epochs=60,\n",
    "                     validation_split=0.2, shuffle=True,\n",
    "                     callbacks=[checkpointer])\n",
    "\n",
    "print()\n",
    "print('Loading best weights...')\n",
    "simple_rnn_model.load_weights(simple_rnn_best_weights)\n",
    "\n",
    "print('Saving model for later use...')\n",
    "simple_rnn_model.save(simple_rnn_file)\n",
    "\n",
    "print()\n",
    "\n",
    "# Print prediction(s)\n",
    "print_predictions(model=simple_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can be used to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "new jersey est parfois chaud en l' mais il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois chaud en l' mais il est parfois parfois en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "paris est parfois parfois en en et il il parfois parfois en à <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois chaud en printemps et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "la fruit préféré moins est la poire mais mon moins aimé est la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "simple_rnn_model = load_model(simple_rnn_file)\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[1], 1))\n",
    "\n",
    "# Print prediction(s)\n",
    "print_predictions(model=simple_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding (IMPLEMENTATION)\n",
    "![RNN](images/embedding.png)\n",
    "You've turned the words into ids, but there's a better representation of a word.  This is called word embeddings.  An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, you'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 21, 7)             1400      \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 21, 14)            924       \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 21, 28)            3612      \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 21, 56)            14280     \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 21, 545)           31065     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 21, 345)           188370    \n",
      "=================================================================\n",
      "Total params: 239,651\n",
      "Trainable params: 239,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.7139 - acc: 0.5955Epoch 00000: val_loss improved from inf to 0.84124, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 23s - loss: 1.7125 - acc: 0.5957 - val_loss: 0.8412 - val_acc: 0.7565\n",
      "Epoch 2/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7398 - acc: 0.7795Epoch 00001: val_loss improved from 0.84124 to 0.52590, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.7396 - acc: 0.7795 - val_loss: 0.5259 - val_acc: 0.8375\n",
      "Epoch 3/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.8254Epoch 00002: val_loss improved from 0.52590 to 0.44530, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.5642 - acc: 0.8255 - val_loss: 0.4453 - val_acc: 0.8610\n",
      "Epoch 4/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8447Epoch 00003: val_loss improved from 0.44530 to 0.41223, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.4917 - acc: 0.8448 - val_loss: 0.4122 - val_acc: 0.8665\n",
      "Epoch 5/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.8575Epoch 00004: val_loss improved from 0.41223 to 0.37791, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.4453 - acc: 0.8575 - val_loss: 0.3779 - val_acc: 0.8758\n",
      "Epoch 6/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8656Epoch 00005: val_loss improved from 0.37791 to 0.35121, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.4159 - acc: 0.8656 - val_loss: 0.3512 - val_acc: 0.8853\n",
      "Epoch 7/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8714Epoch 00006: val_loss improved from 0.35121 to 0.34572, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3951 - acc: 0.8714 - val_loss: 0.3457 - val_acc: 0.8878\n",
      "Epoch 8/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8757Epoch 00007: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3812 - acc: 0.8758 - val_loss: 0.3572 - val_acc: 0.8845\n",
      "Epoch 9/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8789Epoch 00008: val_loss improved from 0.34572 to 0.31624, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3701 - acc: 0.8789 - val_loss: 0.3162 - val_acc: 0.8923\n",
      "Epoch 10/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8818Epoch 00009: val_loss improved from 0.31624 to 0.31397, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3586 - acc: 0.8818 - val_loss: 0.3140 - val_acc: 0.8963\n",
      "Epoch 11/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8847Epoch 00010: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3493 - acc: 0.8847 - val_loss: 0.3186 - val_acc: 0.8919\n",
      "Epoch 12/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8857Epoch 00011: val_loss improved from 0.31397 to 0.30283, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3458 - acc: 0.8857 - val_loss: 0.3028 - val_acc: 0.8998\n",
      "Epoch 13/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8881Epoch 00012: val_loss improved from 0.30283 to 0.29513, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3375 - acc: 0.8881 - val_loss: 0.2951 - val_acc: 0.9001\n",
      "Epoch 14/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8894Epoch 00013: val_loss improved from 0.29513 to 0.29214, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3334 - acc: 0.8894 - val_loss: 0.2921 - val_acc: 0.9015\n",
      "Epoch 15/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8893Epoch 00014: val_loss improved from 0.29214 to 0.28548, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3330 - acc: 0.8892 - val_loss: 0.2855 - val_acc: 0.9042\n",
      "Epoch 16/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8917Epoch 00015: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3258 - acc: 0.8918 - val_loss: 0.2872 - val_acc: 0.9033\n",
      "Epoch 17/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8923Epoch 00016: val_loss improved from 0.28548 to 0.27667, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3235 - acc: 0.8923 - val_loss: 0.2767 - val_acc: 0.9058\n",
      "Epoch 18/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8926Epoch 00017: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3220 - acc: 0.8926 - val_loss: 0.2778 - val_acc: 0.9047\n",
      "Epoch 19/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.8936Epoch 00018: val_loss improved from 0.27667 to 0.27276, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3189 - acc: 0.8936 - val_loss: 0.2728 - val_acc: 0.9075\n",
      "Epoch 20/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8932Epoch 00019: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3215 - acc: 0.8932 - val_loss: 0.2820 - val_acc: 0.9025\n",
      "Epoch 21/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8960Epoch 00020: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3105 - acc: 0.8960 - val_loss: 0.2732 - val_acc: 0.9075\n",
      "Epoch 22/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8940Epoch 00021: val_loss improved from 0.27276 to 0.27250, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3185 - acc: 0.8940 - val_loss: 0.2725 - val_acc: 0.9079\n",
      "Epoch 23/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.8961Epoch 00022: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3111 - acc: 0.8961 - val_loss: 0.2765 - val_acc: 0.9080\n",
      "Epoch 24/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.8978Epoch 00023: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3057 - acc: 0.8978 - val_loss: 0.2739 - val_acc: 0.9080\n",
      "Epoch 25/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3095 - acc: 0.8969Epoch 00024: val_loss improved from 0.27250 to 0.26631, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3094 - acc: 0.8969 - val_loss: 0.2663 - val_acc: 0.9089\n",
      "Epoch 26/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.8981Epoch 00025: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3061 - acc: 0.8980 - val_loss: 0.2680 - val_acc: 0.9095\n",
      "Epoch 27/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.8959Epoch 00026: val_loss improved from 0.26631 to 0.26604, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3125 - acc: 0.8959 - val_loss: 0.2660 - val_acc: 0.9107\n",
      "Epoch 28/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.8970Epoch 00027: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.3090 - acc: 0.8970 - val_loss: 0.2715 - val_acc: 0.9095\n",
      "Epoch 29/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.8991Epoch 00028: val_loss improved from 0.26604 to 0.26598, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.3011 - acc: 0.8991 - val_loss: 0.2660 - val_acc: 0.9095\n",
      "Epoch 30/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.8997Epoch 00029: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2994 - acc: 0.8997 - val_loss: 0.2676 - val_acc: 0.9054\n",
      "Epoch 31/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9009Epoch 00030: val_loss improved from 0.26598 to 0.26552, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2951 - acc: 0.9009 - val_loss: 0.2655 - val_acc: 0.9096\n",
      "Epoch 32/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.9004Epoch 00031: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2984 - acc: 0.9004 - val_loss: 0.2676 - val_acc: 0.9105\n",
      "Epoch 33/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9007Epoch 00032: val_loss improved from 0.26552 to 0.26491, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2975 - acc: 0.9007 - val_loss: 0.2649 - val_acc: 0.9109\n",
      "Epoch 34/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.8996Epoch 00033: val_loss improved from 0.26491 to 0.26369, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2998 - acc: 0.8996 - val_loss: 0.2637 - val_acc: 0.9111\n",
      "Epoch 35/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9020Epoch 00034: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2930 - acc: 0.9020 - val_loss: 0.2682 - val_acc: 0.9097\n",
      "Epoch 36/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9014Epoch 00035: val_loss improved from 0.26369 to 0.26121, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2938 - acc: 0.9015 - val_loss: 0.2612 - val_acc: 0.9118\n",
      "Epoch 37/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9019Epoch 00036: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2930 - acc: 0.9019 - val_loss: 0.2655 - val_acc: 0.9102\n",
      "Epoch 38/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9023Epoch 00037: val_loss improved from 0.26121 to 0.26055, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2921 - acc: 0.9023 - val_loss: 0.2606 - val_acc: 0.9117\n",
      "Epoch 39/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9020Epoch 00038: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2929 - acc: 0.9020 - val_loss: 0.2617 - val_acc: 0.9103\n",
      "Epoch 40/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9032Epoch 00039: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2899 - acc: 0.9032 - val_loss: 0.2610 - val_acc: 0.9123\n",
      "Epoch 41/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9023Epoch 00040: val_loss improved from 0.26055 to 0.25930, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2925 - acc: 0.9023 - val_loss: 0.2593 - val_acc: 0.9121\n",
      "Epoch 42/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9007Epoch 00041: val_loss improved from 0.25930 to 0.25724, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2989 - acc: 0.9007 - val_loss: 0.2572 - val_acc: 0.9141\n",
      "Epoch 43/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9040Epoch 00042: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2858 - acc: 0.9040 - val_loss: 0.2584 - val_acc: 0.9134\n",
      "Epoch 44/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9049Epoch 00043: val_loss improved from 0.25724 to 0.25355, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2833 - acc: 0.9049 - val_loss: 0.2536 - val_acc: 0.9124\n",
      "Epoch 45/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9041Epoch 00044: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2862 - acc: 0.9041 - val_loss: 0.2633 - val_acc: 0.9128\n",
      "Epoch 46/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9030Epoch 00045: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2903 - acc: 0.9030 - val_loss: 0.2635 - val_acc: 0.9112\n",
      "Epoch 47/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.9046Epoch 00046: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2854 - acc: 0.9046 - val_loss: 0.2554 - val_acc: 0.9144\n",
      "Epoch 48/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9052Epoch 00047: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2830 - acc: 0.9052 - val_loss: 0.2572 - val_acc: 0.9131\n",
      "Epoch 49/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9046Epoch 00048: val_loss improved from 0.25355 to 0.25133, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2856 - acc: 0.9046 - val_loss: 0.2513 - val_acc: 0.9153\n",
      "Epoch 50/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9051Epoch 00049: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2840 - acc: 0.9050 - val_loss: 0.2537 - val_acc: 0.9149\n",
      "Epoch 51/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9049Epoch 00050: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2843 - acc: 0.9048 - val_loss: 0.2576 - val_acc: 0.9128\n",
      "Epoch 52/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9054Epoch 00051: val_loss improved from 0.25133 to 0.25084, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2833 - acc: 0.9053 - val_loss: 0.2508 - val_acc: 0.9149\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9056Epoch 00052: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2821 - acc: 0.9056 - val_loss: 0.2581 - val_acc: 0.9132\n",
      "Epoch 54/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9044Epoch 00053: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2871 - acc: 0.9044 - val_loss: 0.2630 - val_acc: 0.9103\n",
      "Epoch 55/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9059Epoch 00054: val_loss improved from 0.25084 to 0.24939, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2816 - acc: 0.9059 - val_loss: 0.2494 - val_acc: 0.9152\n",
      "Epoch 56/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9029Epoch 00055: val_loss improved from 0.24939 to 0.24810, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2928 - acc: 0.9029 - val_loss: 0.2481 - val_acc: 0.9165\n",
      "Epoch 57/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9058Epoch 00056: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2807 - acc: 0.9058 - val_loss: 0.2487 - val_acc: 0.9165\n",
      "Epoch 58/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9066Epoch 00057: val_loss improved from 0.24810 to 0.24238, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2775 - acc: 0.9066 - val_loss: 0.2424 - val_acc: 0.9178\n",
      "Epoch 59/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9058Epoch 00058: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2811 - acc: 0.9058 - val_loss: 0.2534 - val_acc: 0.9154\n",
      "Epoch 60/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9062Epoch 00059: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2806 - acc: 0.9062 - val_loss: 0.2502 - val_acc: 0.9150\n",
      "Epoch 61/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9058Epoch 00060: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2816 - acc: 0.9058 - val_loss: 0.2496 - val_acc: 0.9170\n",
      "Epoch 62/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9057Epoch 00061: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2820 - acc: 0.9057 - val_loss: 0.2567 - val_acc: 0.9143\n",
      "Epoch 63/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9074Epoch 00062: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2766 - acc: 0.9074 - val_loss: 0.2484 - val_acc: 0.9145\n",
      "Epoch 64/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9069Epoch 00063: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2775 - acc: 0.9069 - val_loss: 0.2533 - val_acc: 0.9145\n",
      "Epoch 65/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9067Epoch 00064: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2789 - acc: 0.9067 - val_loss: 0.2459 - val_acc: 0.9169\n",
      "Epoch 66/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9061Epoch 00065: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2813 - acc: 0.9061 - val_loss: 0.2599 - val_acc: 0.9108\n",
      "Epoch 67/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9039Epoch 00066: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2917 - acc: 0.9039 - val_loss: 0.2628 - val_acc: 0.9117\n",
      "Epoch 68/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9066Epoch 00067: val_loss improved from 0.24238 to 0.23949, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2803 - acc: 0.9066 - val_loss: 0.2395 - val_acc: 0.9199\n",
      "Epoch 69/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9085Epoch 00068: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2727 - acc: 0.9085 - val_loss: 0.2467 - val_acc: 0.9185\n",
      "Epoch 70/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9084Epoch 00069: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2725 - acc: 0.9084 - val_loss: 0.2434 - val_acc: 0.9179\n",
      "Epoch 71/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9090Epoch 00070: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2713 - acc: 0.9090 - val_loss: 0.2402 - val_acc: 0.9170\n",
      "Epoch 72/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9081Epoch 00071: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2741 - acc: 0.9081 - val_loss: 0.2464 - val_acc: 0.9143\n",
      "Epoch 73/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9083Epoch 00072: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2730 - acc: 0.9083 - val_loss: 0.2465 - val_acc: 0.9170\n",
      "Epoch 74/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9081Epoch 00073: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2740 - acc: 0.9081 - val_loss: 0.2499 - val_acc: 0.9159\n",
      "Epoch 75/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9083Epoch 00074: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2745 - acc: 0.9083 - val_loss: 0.2469 - val_acc: 0.9168\n",
      "Epoch 76/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9090Epoch 00075: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2704 - acc: 0.9090 - val_loss: 0.2449 - val_acc: 0.9175\n",
      "Epoch 77/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9075Epoch 00076: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2766 - acc: 0.9074 - val_loss: 0.2457 - val_acc: 0.9162\n",
      "Epoch 78/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9083Epoch 00077: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2728 - acc: 0.9084 - val_loss: 0.2506 - val_acc: 0.9153\n",
      "Epoch 79/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9079Epoch 00078: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2748 - acc: 0.9080 - val_loss: 0.2622 - val_acc: 0.9130\n",
      "Epoch 80/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9077Epoch 00079: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2755 - acc: 0.9077 - val_loss: 0.2437 - val_acc: 0.9152\n",
      "Epoch 81/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9083Epoch 00080: val_loss improved from 0.23949 to 0.23899, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2746 - acc: 0.9083 - val_loss: 0.2390 - val_acc: 0.9198\n",
      "Epoch 82/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9092Epoch 00081: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2715 - acc: 0.9092 - val_loss: 0.2460 - val_acc: 0.9180\n",
      "Epoch 83/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9088Epoch 00082: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2726 - acc: 0.9088 - val_loss: 0.2462 - val_acc: 0.9170\n",
      "Epoch 84/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9096Epoch 00083: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2690 - acc: 0.9096 - val_loss: 0.2745 - val_acc: 0.9100\n",
      "Epoch 85/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9066Epoch 00084: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2803 - acc: 0.9066 - val_loss: 0.2428 - val_acc: 0.9183\n",
      "Epoch 86/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9081Epoch 00085: val_loss improved from 0.23899 to 0.23808, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2761 - acc: 0.9081 - val_loss: 0.2381 - val_acc: 0.9207\n",
      "Epoch 87/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9098Epoch 00086: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2689 - acc: 0.9097 - val_loss: 0.2459 - val_acc: 0.9168\n",
      "Epoch 88/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9087Epoch 00087: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2730 - acc: 0.9087 - val_loss: 0.2509 - val_acc: 0.9180\n",
      "Epoch 89/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9070Epoch 00088: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2800 - acc: 0.9070 - val_loss: 0.2513 - val_acc: 0.9163\n",
      "Epoch 90/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9083Epoch 00089: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2741 - acc: 0.9083 - val_loss: 0.2448 - val_acc: 0.9187\n",
      "Epoch 91/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9099Epoch 00090: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2699 - acc: 0.9098 - val_loss: 0.2430 - val_acc: 0.9185\n",
      "Epoch 92/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9088- ETAEpoch 00091: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2739 - acc: 0.9088 - val_loss: 0.2478 - val_acc: 0.9169\n",
      "Epoch 93/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9090Epoch 00092: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2718 - acc: 0.9090 - val_loss: 0.2395 - val_acc: 0.9186\n",
      "Epoch 94/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9099Epoch 00093: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2698 - acc: 0.9098 - val_loss: 0.2457 - val_acc: 0.9175\n",
      "Epoch 95/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9088Epoch 00094: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2730 - acc: 0.9088 - val_loss: 0.2434 - val_acc: 0.9187\n",
      "Epoch 96/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9098Epoch 00095: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2688 - acc: 0.9098 - val_loss: 0.2420 - val_acc: 0.9188\n",
      "Epoch 97/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9105Epoch 00096: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2663 - acc: 0.9105 - val_loss: 0.2438 - val_acc: 0.9172\n",
      "Epoch 98/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9084Epoch 00097: val_loss did not improve\n",
      "110288/110288 [==============================] - 22s - loss: 0.2753 - acc: 0.9084 - val_loss: 0.2481 - val_acc: 0.9171\n",
      "Epoch 99/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9095Epoch 00098: val_loss improved from 0.23808 to 0.23786, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2700 - acc: 0.9095 - val_loss: 0.2379 - val_acc: 0.9208\n",
      "Epoch 100/100\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9096Epoch 00099: val_loss improved from 0.23786 to 0.23729, saving model to embed_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 22s - loss: 0.2707 - acc: 0.9096 - val_loss: 0.2373 - val_acc: 0.9205\n",
      "\n",
      "Loading best weights\n",
      "Saving embed model for later usage.\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est généralement froid en juillet et il est généralement en l' <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "californie est généralement calme en mois et il est généralement chaud en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois doux en juin et il est froid en septembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "votre fruit aimé fruit fruits est raisin mais mais moins aimé aimé est la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "     \n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    embed = Embedding(english_vocab_size, int(input_shape[1]/3))(inputs)\n",
    "    gru0 = GRU(14, return_sequences=True, dropout=0.1)(embed)\n",
    "    gru1 = GRU(28, return_sequences=True, dropout=0.2)(gru0)\n",
    "    gru2 = GRU(56, return_sequences=True, dropout=0.4)(gru1)\n",
    "    dense0 = TimeDistributed(Dense(english_vocab_size + french_vocab_size, activation='tanh'))(gru2)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(dense0)\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tests.test_embed_model(embed_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "# tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[1]))\n",
    "\n",
    "# TODO: Train the neural network\n",
    "embed_rnn_model = embed_model(input_shape=tmp_x.shape,\n",
    "                              output_sequence_length=preproc_french_sentences[1],\n",
    "                              english_vocab_size=len(english_tokenizer.word_index) + 1,\n",
    "                              french_vocab_size=len(french_tokenizer.word_index) + 1)\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "checkpointer =  ModelCheckpoint(filepath=embed_rnn_best_weights, save_best_only=True, verbose=1)\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=512,\n",
    "                    epochs=100, validation_split=0.2,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "print()\n",
    "print('Loading best weights')\n",
    "embed_rnn_model.load_weights(embed_rnn_best_weights)\n",
    "print('Saving embed model for later usage.')\n",
    "embed_rnn_model.save(embed_rnn_file)\n",
    "\n",
    "# TODO: Print prediction(s)\n",
    "print_predictions(model=embed_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell can be used to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est généralement froid en juillet et il est généralement en l' <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "californie est généralement calme en mois et il est généralement chaud en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois doux en juin et il est froid en septembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "votre fruit aimé fruit fruits est raisin mais mais moins aimé aimé est la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "embed_rnn_model = load_model(embed_rnn_file)\n",
    "\n",
    "# Preprocess inputs\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# Print predictions\n",
    "print_predictions(model=embed_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs (IMPLEMENTATION)\n",
    "![RNN](images/bidirectional.png)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 21, 28)            1344      \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 21, 56)            9576      \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 21, 112)           37968     \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 21, 545)           61585     \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 21, 345)           188370    \n",
      "=================================================================\n",
      "Total params: 298,843\n",
      "Trainable params: 298,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.9928 - acc: 0.5461Epoch 00000: val_loss improved from inf to 1.34589, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 27s - loss: 1.9886 - acc: 0.5467 - val_loss: 1.3459 - val_acc: 0.6284\n",
      "Epoch 2/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.2262 - acc: 0.6446Epoch 00001: val_loss improved from 1.34589 to 1.11177, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 1.2255 - acc: 0.6448 - val_loss: 1.1118 - val_acc: 0.6640\n",
      "Epoch 3/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 1.0835 - acc: 0.6683Epoch 00002: val_loss improved from 1.11177 to 1.01723, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 1.0831 - acc: 0.6684 - val_loss: 1.0172 - val_acc: 0.6813\n",
      "Epoch 4/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9926 - acc: 0.6837Epoch 00003: val_loss improved from 1.01723 to 1.00662, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.9924 - acc: 0.6837 - val_loss: 1.0066 - val_acc: 0.6683\n",
      "Epoch 5/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9477 - acc: 0.6916Epoch 00004: val_loss improved from 1.00662 to 0.88379, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.9473 - acc: 0.6917 - val_loss: 0.8838 - val_acc: 0.7052\n",
      "Epoch 6/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.9023 - acc: 0.6993Epoch 00005: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.9019 - acc: 0.6994 - val_loss: 0.9070 - val_acc: 0.7043\n",
      "Epoch 7/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.7101Epoch 00006: val_loss improved from 0.88379 to 0.75875, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.8575 - acc: 0.7103 - val_loss: 0.7587 - val_acc: 0.7462\n",
      "Epoch 8/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.8280 - acc: 0.7233Epoch 00007: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.8276 - acc: 0.7234 - val_loss: 0.7634 - val_acc: 0.7436\n",
      "Epoch 9/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7555 - acc: 0.7437Epoch 00008: val_loss improved from 0.75875 to 0.69923, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.7553 - acc: 0.7437 - val_loss: 0.6992 - val_acc: 0.7608\n",
      "Epoch 10/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7573 - acc: 0.7475Epoch 00009: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.7582 - acc: 0.7472 - val_loss: 0.9243 - val_acc: 0.7102\n",
      "Epoch 11/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7400 - acc: 0.7477Epoch 00010: val_loss improved from 0.69923 to 0.64409, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.7393 - acc: 0.7479 - val_loss: 0.6441 - val_acc: 0.7755\n",
      "Epoch 12/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.7527 - acc: 0.7449Epoch 00011: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.7523 - acc: 0.7449 - val_loss: 0.7027 - val_acc: 0.7486\n",
      "Epoch 13/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6824 - acc: 0.7573Epoch 00012: val_loss improved from 0.64409 to 0.59827, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.6820 - acc: 0.7576 - val_loss: 0.5983 - val_acc: 0.7913\n",
      "Epoch 14/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6486 - acc: 0.7698Epoch 00013: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.6484 - acc: 0.7699 - val_loss: 0.6163 - val_acc: 0.7829\n",
      "Epoch 15/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6310 - acc: 0.7804Epoch 00014: val_loss improved from 0.59827 to 0.53547, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.6302 - acc: 0.7807 - val_loss: 0.5355 - val_acc: 0.8178\n",
      "Epoch 16/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5830 - acc: 0.7953Epoch 00015: val_loss improved from 0.53547 to 0.51004, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.5824 - acc: 0.7955 - val_loss: 0.5100 - val_acc: 0.8237\n",
      "Epoch 17/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6265 - acc: 0.7833Epoch 00016: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.6262 - acc: 0.7834 - val_loss: 0.5619 - val_acc: 0.7990\n",
      "Epoch 18/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.8012Epoch 00017: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.5574 - acc: 0.8012 - val_loss: 0.6749 - val_acc: 0.7583\n",
      "Epoch 19/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.6009 - acc: 0.7895Epoch 00018: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.6004 - acc: 0.7896 - val_loss: 0.5178 - val_acc: 0.8163\n",
      "Epoch 20/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8226Epoch 00019: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4993 - acc: 0.8226 - val_loss: 0.6315 - val_acc: 0.7784\n",
      "Epoch 21/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.7951Epoch 00020: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.5842 - acc: 0.7953 - val_loss: 0.5559 - val_acc: 0.7970\n",
      "Epoch 22/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8206Epoch 00021: val_loss improved from 0.51004 to 0.46579, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4939 - acc: 0.8207 - val_loss: 0.4658 - val_acc: 0.8306\n",
      "Epoch 23/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8291Epoch 00022: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4782 - acc: 0.8286 - val_loss: 0.7658 - val_acc: 0.7689\n",
      "Epoch 24/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8245Epoch 00023: val_loss improved from 0.46579 to 0.45430, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4868 - acc: 0.8245 - val_loss: 0.4543 - val_acc: 0.8339\n",
      "Epoch 25/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.8203Epoch 00024: val_loss improved from 0.45430 to 0.41412, saving model to db_rnn_best_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 24s - loss: 0.4971 - acc: 0.8206 - val_loss: 0.4141 - val_acc: 0.8531\n",
      "Epoch 26/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4933 - acc: 0.8226Epoch 00025: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4926 - acc: 0.8228 - val_loss: 0.4349 - val_acc: 0.8384\n",
      "Epoch 27/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8305Epoch 00026: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4853 - acc: 0.8308 - val_loss: 0.4407 - val_acc: 0.8410\n",
      "Epoch 28/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8346Epoch 00027: val_loss improved from 0.41412 to 0.38512, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4649 - acc: 0.8347 - val_loss: 0.3851 - val_acc: 0.8649\n",
      "Epoch 29/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.8074Epoch 00028: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.5710 - acc: 0.8072 - val_loss: 0.6265 - val_acc: 0.7825\n",
      "Epoch 30/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.8289Epoch 00029: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4732 - acc: 0.8289 - val_loss: 0.5799 - val_acc: 0.7939\n",
      "Epoch 31/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.8059Epoch 00030: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.5618 - acc: 0.8061 - val_loss: 0.4401 - val_acc: 0.8468\n",
      "Epoch 32/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4449 - acc: 0.8422Epoch 00031: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4449 - acc: 0.8421 - val_loss: 0.4641 - val_acc: 0.8313\n",
      "Epoch 33/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8415Epoch 00032: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4414 - acc: 0.8415 - val_loss: 0.4287 - val_acc: 0.8374\n",
      "Epoch 34/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8337Epoch 00033: val_loss improved from 0.38512 to 0.36684, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4676 - acc: 0.8340 - val_loss: 0.3668 - val_acc: 0.8706\n",
      "Epoch 35/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.8283Epoch 00034: val_loss improved from 0.36684 to 0.36406, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4804 - acc: 0.8285 - val_loss: 0.3641 - val_acc: 0.8718\n",
      "Epoch 36/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8364Epoch 00035: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4679 - acc: 0.8366 - val_loss: 0.3936 - val_acc: 0.8543\n",
      "Epoch 37/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8454Epoch 00036: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4349 - acc: 0.8455 - val_loss: 0.3741 - val_acc: 0.8649\n",
      "Epoch 38/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4780 - acc: 0.8304Epoch 00037: val_loss improved from 0.36406 to 0.36312, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4772 - acc: 0.8307 - val_loss: 0.3631 - val_acc: 0.8713\n",
      "Epoch 39/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8458Epoch 00038: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4201 - acc: 0.8459 - val_loss: 0.3853 - val_acc: 0.8588\n",
      "Epoch 40/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8422Epoch 00039: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4366 - acc: 0.8418 - val_loss: 0.4471 - val_acc: 0.8339\n",
      "Epoch 41/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8699Epoch 00040: val_loss improved from 0.36312 to 0.33231, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.3562 - acc: 0.8700 - val_loss: 0.3323 - val_acc: 0.8823\n",
      "Epoch 42/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8420Epoch 00041: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4374 - acc: 0.8421 - val_loss: 0.4041 - val_acc: 0.8472\n",
      "Epoch 43/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8559Epoch 00042: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3957 - acc: 0.8555 - val_loss: 0.5692 - val_acc: 0.7981\n",
      "Epoch 44/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8579Epoch 00043: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3858 - acc: 0.8579 - val_loss: 0.3694 - val_acc: 0.8620\n",
      "Epoch 45/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8722Epoch 00044: val_loss improved from 0.33231 to 0.33171, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.3403 - acc: 0.8723 - val_loss: 0.3317 - val_acc: 0.8777\n",
      "Epoch 46/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8476Epoch 00045: val_loss improved from 0.33171 to 0.31785, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.4390 - acc: 0.8478 - val_loss: 0.3178 - val_acc: 0.8868\n",
      "Epoch 47/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8522Epoch 00046: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4170 - acc: 0.8524 - val_loss: 0.3339 - val_acc: 0.8759\n",
      "Epoch 48/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8575Epoch 00047: val_loss improved from 0.31785 to 0.31724, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.3901 - acc: 0.8577 - val_loss: 0.3172 - val_acc: 0.8832\n",
      "Epoch 49/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8730Epoch 00048: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3436 - acc: 0.8731 - val_loss: 0.3316 - val_acc: 0.8810\n",
      "Epoch 50/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8333Epoch 00049: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4969 - acc: 0.8331 - val_loss: 0.4581 - val_acc: 0.8371\n",
      "Epoch 51/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.8477Epoch 00050: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4252 - acc: 0.8478 - val_loss: 0.3667 - val_acc: 0.8628\n",
      "Epoch 52/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8744Epoch 00051: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3517 - acc: 0.8740 - val_loss: 0.5213 - val_acc: 0.8170\n",
      "Epoch 53/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3699 - acc: 0.8652Epoch 00052: val_loss improved from 0.31724 to 0.29293, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.3694 - acc: 0.8655 - val_loss: 0.2929 - val_acc: 0.8961\n",
      "Epoch 54/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8594Epoch 00053: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4042 - acc: 0.8594 - val_loss: 0.3700 - val_acc: 0.8651\n",
      "Epoch 55/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8534Epoch 00054: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4227 - acc: 0.8536 - val_loss: 0.3325 - val_acc: 0.8789\n",
      "Epoch 56/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8520Epoch 00055: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.4260 - acc: 0.8522 - val_loss: 0.3155 - val_acc: 0.8868\n",
      "Epoch 57/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3888 - acc: 0.8641Epoch 00056: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3888 - acc: 0.8640 - val_loss: 0.4030 - val_acc: 0.8578\n",
      "Epoch 58/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8591Epoch 00057: val_loss improved from 0.29293 to 0.29026, saving model to db_rnn_best_weights.hdf5\n",
      "110288/110288 [==============================] - 24s - loss: 0.3919 - acc: 0.8594 - val_loss: 0.2903 - val_acc: 0.8986\n",
      "Epoch 59/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8679Epoch 00058: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3723 - acc: 0.8678 - val_loss: 0.4915 - val_acc: 0.8165\n",
      "Epoch 60/60\n",
      "109568/110288 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8754Epoch 00059: val_loss did not improve\n",
      "110288/110288 [==============================] - 24s - loss: 0.3403 - acc: 0.8756 - val_loss: 0.2964 - val_acc: 0.8934\n",
      "\n",
      "Loading the best weights\n",
      "\n",
      "Saving the model for later usage...\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est généralement froid en juillet et il gèle habituellement en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "california est généralement calme en mars et il est généralement chaud en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois doux en juin et il est froid en septembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "votre moins aimé fruit est la raisin mais mon moins aimé est la pomme <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    gru0 = Bidirectional(GRU(14, return_sequences=True))(inputs)\n",
    "    gru1 = Bidirectional(GRU(28, return_sequences=True))(gru0)\n",
    "    gru2 = Bidirectional(GRU(56, return_sequences=True))(gru1)\n",
    "    dense0 = TimeDistributed(Dense(english_vocab_size + french_vocab_size, activation='tanh'))(gru2)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(dense0)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tests.test_bd_model(bd_model)\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[1], 1))\n",
    "\n",
    "bd_rnn_model = bd_model(input_shape=tmp_x.shape,\n",
    "                        output_sequence_length=preproc_english_sentences.shape[1],\n",
    "                        english_vocab_size=len(english_tokenizer.word_index) + 1,\n",
    "                        french_vocab_size=len(french_tokenizer.word_index) + 1)\n",
    "\n",
    "bd_rnn_model.summary()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=bd_rnn_best_weights, save_best_only=True, verbose=1)\n",
    "\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024,\n",
    "                 epochs=60, validation_split=0.2,\n",
    "                 callbacks=[checkpointer])\n",
    "\n",
    "# Loading best weights and saving the model\n",
    "print()\n",
    "print('Loading the best weights')\n",
    "bd_rnn_model.load_weights(bd_rnn_best_weights)\n",
    "print()\n",
    "print('Saving the model for later usage...')\n",
    "bd_rnn_model.save(bd_rnn_file)\n",
    "\n",
    "# Printing and comparing predictions\n",
    "print_predictions(model=bd_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell can be used to load the model and print some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est généralement froid en juillet et il gèle habituellement en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "california est généralement calme en mars et il est généralement chaud en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois doux en juin et il est froid en septembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "votre moins aimé fruit est la raisin mais mon moins aimé est la pomme <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "bd_rnn_model = load_model(bd_rnn_file)\n",
    "\n",
    "# Preprocessing inputs\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[1], 1))\n",
    "\n",
    "# Print predictions\n",
    "print_predictions(model=bd_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder (OPTIONAL)\n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_71 (GRU)                 (None, 200)               121200    \n",
      "_________________________________________________________________\n",
      "repeat_vector_23 (RepeatVect (None, 21, 200)           0         \n",
      "_________________________________________________________________\n",
      "gru_72 (GRU)                 (None, 21, 200)           240600    \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, 21, 345)           69345     \n",
      "=================================================================\n",
      "Total params: 431,145\n",
      "Trainable params: 431,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 103395 samples, validate on 34466 samples\n",
      "Epoch 1/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 2.7429 - acc: 0.4506Epoch 00000: val_loss improved from inf to 2.13310, saving model to autoenc_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 22s - loss: 2.7374 - acc: 0.4512 - val_loss: 2.1331 - val_acc: 0.5138\n",
      "Epoch 2/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.9860 - acc: 0.5293Epoch 00001: val_loss improved from 2.13310 to 1.67370, saving model to autoenc_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 18s - loss: 1.9831 - acc: 0.5296 - val_loss: 1.6737 - val_acc: 0.5661\n",
      "Epoch 3/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.6221 - acc: 0.5737Epoch 00002: val_loss improved from 1.67370 to 1.55111, saving model to autoenc_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 18s - loss: 1.6217 - acc: 0.5738 - val_loss: 1.5511 - val_acc: 0.5893\n",
      "Epoch 4/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.4742 - acc: 0.5989Epoch 00003: val_loss improved from 1.55111 to 1.42969, saving model to autoenc_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 18s - loss: 1.4736 - acc: 0.5990 - val_loss: 1.4297 - val_acc: 0.6032\n",
      "Epoch 5/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.3856 - acc: 0.6143Epoch 00004: val_loss improved from 1.42969 to 1.39182, saving model to autoenc_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 18s - loss: 1.3855 - acc: 0.6143 - val_loss: 1.3918 - val_acc: 0.6137\n",
      "Epoch 6/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.3099 - acc: 0.6297Epoch 00005: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.3096 - acc: 0.6297 - val_loss: 1.4476 - val_acc: 0.6115\n",
      "Epoch 7/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2729 - acc: 0.6361Epoch 00006: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2728 - acc: 0.6361 - val_loss: 1.4713 - val_acc: 0.6068\n",
      "Epoch 8/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2810 - acc: 0.6340Epoch 00007: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2817 - acc: 0.6339 - val_loss: 1.4409 - val_acc: 0.6096\n",
      "Epoch 9/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2525 - acc: 0.6403Epoch 00008: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2522 - acc: 0.6404 - val_loss: 1.5561 - val_acc: 0.5990\n",
      "Epoch 10/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2510 - acc: 0.6395Epoch 00009: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2509 - acc: 0.6395 - val_loss: 1.5558 - val_acc: 0.6041\n",
      "Epoch 11/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2152 - acc: 0.6481Epoch 00010: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2155 - acc: 0.6481 - val_loss: 1.5437 - val_acc: 0.6018\n",
      "Epoch 12/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2376 - acc: 0.6449Epoch 00011: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2374 - acc: 0.6450 - val_loss: 1.6126 - val_acc: 0.5940\n",
      "Epoch 13/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2147 - acc: 0.6482Epoch 00012: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2147 - acc: 0.6483 - val_loss: 1.4824 - val_acc: 0.6160\n",
      "Epoch 14/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2354 - acc: 0.6442Epoch 00013: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2354 - acc: 0.6442 - val_loss: 1.4104 - val_acc: 0.6106\n",
      "Epoch 15/15\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2243 - acc: 0.6446Epoch 00014: val_loss did not improve\n",
      "103395/103395 [==============================] - 18s - loss: 1.2243 - acc: 0.6446 - val_loss: 1.5102 - val_acc: 0.6046\n",
      "Loading best weights...\n",
      "Saving model...\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est chaud en mois mais il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois chaud en mois mais il est est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "paris est est parfois en en et il il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois chaud en mois mais il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "son fruit le plus est la la mais mais moins moins est la la <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector\n",
    "\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    \n",
    "    # Encoder part\n",
    "    enc_gru0 = GRU(200, dropout=0.2)(inputs)\n",
    "    \n",
    "    # Decoder part\n",
    "    repeat = RepeatVector(output_sequence_length)(enc_gru0)\n",
    "    dec_gru0 = GRU(200, return_sequences=True, dropout=0.2)(repeat)\n",
    "    \n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(dec_gru0)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tests.test_encdec_model(encdec_model)\n",
    "\n",
    "\n",
    "# OPTIONAL: Train and Print prediction(s)\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[1], 1))\n",
    "\n",
    "encdec_rnn_model = encdec_model(tmp_x.shape,\n",
    "                                preproc_french_sentences.shape[1],\n",
    "                                len(english_tokenizer.word_index.keys()) + 1,\n",
    "                                len(french_tokenizer.word_index.keys()) + 1)\n",
    "\n",
    "encdec_rnn_model.summary()\n",
    "\n",
    "# Training the model\n",
    "checkpointer = ModelCheckpoint(filepath=encdec_rnn_best_weights, save_best_only=True, verbose=1)\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x,\n",
    "                     preproc_french_sentences,\n",
    "                     batch_size=1024, epochs=15, validation_split=0.25,\n",
    "                     callbacks=[checkpointer])\n",
    "\n",
    "# Loading best weights\n",
    "print('Loading best weights...')\n",
    "encdec_rnn_model.load_weights(encdec_rnn_best_weights)\n",
    "\n",
    "# Saving model for later use\n",
    "print('Saving model...')\n",
    "encdec_rnn_model.save(encdec_rnn_file)\n",
    "\n",
    "# Printing and comparing predictions\n",
    "print_predictions(model=encdec_rnn_model, X=tmp_x,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # ------- INPUT LAYER -------\n",
    "    inputs =  Input(shape=input_shape[1:])\n",
    "    # ------- ERMBEDDING LAYER -------\n",
    "    embed = Embedding(input_dim=english_vocab_size, output_dim=60)(inputs)\n",
    "    # ------- ENCODER GRU LAYER 0 -------\n",
    "    enc_gru0 = Bidirectional(GRU(200, dropout=0.35))(embed)\n",
    "    # ------- REPEAT LAYER -------\n",
    "    repeat = RepeatVector(output_sequence_length)(enc_gru0)\n",
    "    # ------- DECODER GRU LAYER 0 -------\n",
    "    dec_gru0 = Bidirectional(GRU(200, return_sequences=True, dropout=0.35))(repeat)\n",
    "    # ------- DENSE LAYER 0 -------\n",
    "    dense1 = TimeDistributed(Dense(256, activation='relu'))(dec_gru0)\n",
    "    dense1 = Dropout(0.1)(dense1)\n",
    "    # ------- OUTPUT LAYER -------\n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(dense1)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 15, 60)            12000     \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 400)               313200    \n",
      "_________________________________________________________________\n",
      "repeat_vector_7 (RepeatVecto (None, 21, 400)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 21, 400)           721200    \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 21, 256)           102656    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 21, 345)           88665     \n",
      "=================================================================\n",
      "Total params: 1,237,721\n",
      "Trainable params: 1,237,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 103395 samples, validate on 34466 samples\n",
      "Epoch 1/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 2.9962 - acc: 0.4425Epoch 00000: val_loss improved from inf to 2.28546, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 37s - loss: 2.9897 - acc: 0.4431 - val_loss: 2.2855 - val_acc: 0.5000\n",
      "Epoch 2/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.8929 - acc: 0.5332Epoch 00001: val_loss improved from 2.28546 to 1.53882, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 1.8906 - acc: 0.5336 - val_loss: 1.5388 - val_acc: 0.5945\n",
      "Epoch 3/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.4883 - acc: 0.6010Epoch 00002: val_loss improved from 1.53882 to 1.30985, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 1.4874 - acc: 0.6012 - val_loss: 1.3098 - val_acc: 0.6457\n",
      "Epoch 4/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.2924 - acc: 0.6453Epoch 00003: val_loss improved from 1.30985 to 1.14660, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 1.2917 - acc: 0.6454 - val_loss: 1.1466 - val_acc: 0.6786\n",
      "Epoch 5/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.1708 - acc: 0.6719Epoch 00004: val_loss improved from 1.14660 to 1.05650, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 1.1706 - acc: 0.6719 - val_loss: 1.0565 - val_acc: 0.6992\n",
      "Epoch 6/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.0869 - acc: 0.6889Epoch 00005: val_loss improved from 1.05650 to 0.99843, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 1.0864 - acc: 0.6890 - val_loss: 0.9984 - val_acc: 0.7107\n",
      "Epoch 7/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 1.0325 - acc: 0.6987Epoch 00006: val_loss improved from 0.99843 to 0.92958, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 1.0320 - acc: 0.6988 - val_loss: 0.9296 - val_acc: 0.7228\n",
      "Epoch 8/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.9697 - acc: 0.7099Epoch 00007: val_loss improved from 0.92958 to 0.88036, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.9696 - acc: 0.7100 - val_loss: 0.8804 - val_acc: 0.7311\n",
      "Epoch 9/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.7168Epoch 00008: val_loss improved from 0.88036 to 0.84417, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.9321 - acc: 0.7168 - val_loss: 0.8442 - val_acc: 0.7400\n",
      "Epoch 10/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7248Epoch 00009: val_loss improved from 0.84417 to 0.82215, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.8926 - acc: 0.7249 - val_loss: 0.8221 - val_acc: 0.7430\n",
      "Epoch 11/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.8511 - acc: 0.7347Epoch 00010: val_loss improved from 0.82215 to 0.77178, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.8507 - acc: 0.7348 - val_loss: 0.7718 - val_acc: 0.7580\n",
      "Epoch 12/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.8077 - acc: 0.7466Epoch 00011: val_loss improved from 0.77178 to 0.72175, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.8074 - acc: 0.7467 - val_loss: 0.7218 - val_acc: 0.7726\n",
      "Epoch 13/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7571Epoch 00012: val_loss improved from 0.72175 to 0.69090, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.7715 - acc: 0.7570 - val_loss: 0.6909 - val_acc: 0.7796\n",
      "Epoch 14/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7684Epoch 00013: val_loss improved from 0.69090 to 0.63673, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.7312 - acc: 0.7685 - val_loss: 0.6367 - val_acc: 0.8003\n",
      "Epoch 15/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.6891 - acc: 0.7819Epoch 00014: val_loss improved from 0.63673 to 0.59640, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.6890 - acc: 0.7820 - val_loss: 0.5964 - val_acc: 0.8141\n",
      "Epoch 16/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.7942Epoch 00015: val_loss improved from 0.59640 to 0.54981, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.6501 - acc: 0.7943 - val_loss: 0.5498 - val_acc: 0.8280\n",
      "Epoch 17/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.6021 - acc: 0.8090Epoch 00016: val_loss improved from 0.54981 to 0.50293, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.6021 - acc: 0.8090 - val_loss: 0.5029 - val_acc: 0.8462\n",
      "Epoch 18/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8233Epoch 00017: val_loss improved from 0.50293 to 0.45179, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.5578 - acc: 0.8234 - val_loss: 0.4518 - val_acc: 0.8615\n",
      "Epoch 19/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.8367Epoch 00018: val_loss improved from 0.45179 to 0.41084, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.5164 - acc: 0.8367 - val_loss: 0.4108 - val_acc: 0.8757\n",
      "Epoch 20/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.4821 - acc: 0.8483Epoch 00019: val_loss improved from 0.41084 to 0.38579, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.4820 - acc: 0.8484 - val_loss: 0.3858 - val_acc: 0.8860\n",
      "Epoch 21/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8584Epoch 00020: val_loss improved from 0.38579 to 0.35439, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.4511 - acc: 0.8586 - val_loss: 0.3544 - val_acc: 0.8947\n",
      "Epoch 22/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8681Epoch 00021: val_loss improved from 0.35439 to 0.32490, saving model to final_rnn_best_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103395/103395 [==============================] - 35s - loss: 0.4214 - acc: 0.8682 - val_loss: 0.3249 - val_acc: 0.9047\n",
      "Epoch 23/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8774Epoch 00022: val_loss improved from 0.32490 to 0.29794, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.3907 - acc: 0.8774 - val_loss: 0.2979 - val_acc: 0.9123\n",
      "Epoch 24/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8874Epoch 00023: val_loss improved from 0.29794 to 0.26216, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.3585 - acc: 0.8874 - val_loss: 0.2622 - val_acc: 0.9228\n",
      "Epoch 25/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8946Epoch 00024: val_loss improved from 0.26216 to 0.26063, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.3348 - acc: 0.8945 - val_loss: 0.2606 - val_acc: 0.9222\n",
      "Epoch 26/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9019Epoch 00025: val_loss improved from 0.26063 to 0.22519, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.3117 - acc: 0.9020 - val_loss: 0.2252 - val_acc: 0.9345\n",
      "Epoch 27/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9080Epoch 00026: val_loss improved from 0.22519 to 0.22264, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2936 - acc: 0.9080 - val_loss: 0.2226 - val_acc: 0.9329\n",
      "Epoch 28/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9130Epoch 00027: val_loss improved from 0.22264 to 0.19793, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2778 - acc: 0.9130 - val_loss: 0.1979 - val_acc: 0.9417\n",
      "Epoch 29/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2568 - acc: 0.9196Epoch 00028: val_loss improved from 0.19793 to 0.18628, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2570 - acc: 0.9195 - val_loss: 0.1863 - val_acc: 0.9451\n",
      "Epoch 30/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9234Epoch 00029: val_loss improved from 0.18628 to 0.17677, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2442 - acc: 0.9234 - val_loss: 0.1768 - val_acc: 0.9476\n",
      "Epoch 31/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9270Epoch 00030: val_loss improved from 0.17677 to 0.16546, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2324 - acc: 0.9270 - val_loss: 0.1655 - val_acc: 0.9505\n",
      "Epoch 32/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9306Epoch 00031: val_loss improved from 0.16546 to 0.16216, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2207 - acc: 0.9306 - val_loss: 0.1622 - val_acc: 0.9505\n",
      "Epoch 33/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9277Epoch 00032: val_loss improved from 0.16216 to 0.15483, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2292 - acc: 0.9277 - val_loss: 0.1548 - val_acc: 0.9525\n",
      "Epoch 34/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9360Epoch 00033: val_loss improved from 0.15483 to 0.15275, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.2025 - acc: 0.9361 - val_loss: 0.1527 - val_acc: 0.9524\n",
      "Epoch 35/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9380Epoch 00034: val_loss improved from 0.15275 to 0.14195, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1963 - acc: 0.9380 - val_loss: 0.1420 - val_acc: 0.9561\n",
      "Epoch 36/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9409Epoch 00035: val_loss improved from 0.14195 to 0.13355, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1868 - acc: 0.9409 - val_loss: 0.1336 - val_acc: 0.9589\n",
      "Epoch 37/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9440Epoch 00036: val_loss did not improve\n",
      "103395/103395 [==============================] - 35s - loss: 0.1771 - acc: 0.9439 - val_loss: 0.1346 - val_acc: 0.9578\n",
      "Epoch 38/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9446Epoch 00037: val_loss improved from 0.13355 to 0.12914, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1749 - acc: 0.9446 - val_loss: 0.1291 - val_acc: 0.9597\n",
      "Epoch 39/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9477Epoch 00038: val_loss improved from 0.12914 to 0.11788, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1647 - acc: 0.9477 - val_loss: 0.1179 - val_acc: 0.9634\n",
      "Epoch 40/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9497Epoch 00039: val_loss did not improve\n",
      "103395/103395 [==============================] - 35s - loss: 0.1582 - acc: 0.9497 - val_loss: 0.1194 - val_acc: 0.9629\n",
      "Epoch 41/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9503Epoch 00040: val_loss improved from 0.11788 to 0.11329, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1559 - acc: 0.9504 - val_loss: 0.1133 - val_acc: 0.9653\n",
      "Epoch 42/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9527Epoch 00041: val_loss did not improve\n",
      "103395/103395 [==============================] - 35s - loss: 0.1484 - acc: 0.9528 - val_loss: 0.1147 - val_acc: 0.9642\n",
      "Epoch 43/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9539Epoch 00042: val_loss improved from 0.11329 to 0.10510, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1447 - acc: 0.9539 - val_loss: 0.1051 - val_acc: 0.9675\n",
      "Epoch 44/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9561Epoch 00043: val_loss improved from 0.10510 to 0.10292, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1374 - acc: 0.9561 - val_loss: 0.1029 - val_acc: 0.9679\n",
      "Epoch 45/45\n",
      "102400/103395 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9566Epoch 00044: val_loss improved from 0.10292 to 0.10043, saving model to final_rnn_best_weights.hdf5\n",
      "103395/103395 [==============================] - 35s - loss: 0.1352 - acc: 0.9567 - val_loss: 0.1004 - val_acc: 0.9685\n",
      "Loading best weights...\n",
      "Saving the model for later use\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est généralement froid en juillet et il gèle habituellement en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "california est généralement calme en mars et il est généralement chaud en juin <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "les états unis est parfois doux en juin et il fait froid en septembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "-------------------------------------------------------\n",
      "\n",
      "Predicted sentence:\n",
      "votre moins aimé moins est le raisin mais mon moins aimé est la pomme <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct translation:\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n"
     ]
    }
   ],
   "source": [
    "final_rnn_model = model_final(preproc_english_sentences.shape,\n",
    "                              preproc_french_sentences.shape[1],\n",
    "                              len(english_tokenizer.word_index.keys()) + 1,\n",
    "                              len(french_tokenizer.word_index.keys()) + 1)\n",
    "\n",
    "final_rnn_model.summary()\n",
    "\n",
    "checkpointer =  ModelCheckpoint(final_rnn_best_weights, save_best_only=True, verbose=2)\n",
    "\n",
    "final_rnn_model.fit(preproc_english_sentences,\n",
    "                    preproc_french_sentences,\n",
    "                    batch_size=1024, epochs=45, validation_split=0.25,\n",
    "                    callbacks=[checkpointer])\n",
    "\n",
    "# Loading the best weigts\n",
    "print('Loading best weights...')\n",
    "final_rnn_model.load_weights(final_rnn_best_weights)\n",
    "\n",
    "# Saving the model\n",
    "print('Saving the model for later use')\n",
    "final_rnn_model.save(final_rnn_file)\n",
    "\n",
    "# Printing some results\n",
    "print_predictions(model=final_rnn_model, X=preproc_english_sentences,\n",
    "                  y=french_sentences, tokenizer=french_tokenizer, number_of_predictions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = load_model(final_rnn_file)\n",
    "\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you are ready to submit your project, do the following steps:\n",
    "1. Ensure you pass all points on the [rubric](https://review.udacity.com/#!/rubrics/1004/view).\n",
    "2. Submit the following in a zip file.\n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
